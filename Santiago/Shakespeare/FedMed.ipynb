{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FedMed.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TWDjzbCzfUFt",
        "Y-ZegBArf4xQ",
        "cce_-qnxhD4n",
        "XelbyPsDlfgb",
        "sWVOxcAao2_t",
        "vFFAfTOwpk4j",
        "VQ9PZM0Gp9ve",
        "YXtGLkoAqLIW"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6d797b2da75d40cc9c17a482bfa2733c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9e037649af1b47d5887e494f42df9e23",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_09e8bd023b73490a82de6a39fa2c3262",
              "IPY_MODEL_63f2767d964f453cb3fb83ffec6208e6"
            ]
          }
        },
        "9e037649af1b47d5887e494f42df9e23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "09e8bd023b73490a82de6a39fa2c3262": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f9e21bf730834d5f81d3254dde399915",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 14,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 14,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_60ec823588c9427cad46b6d0fa074477"
          }
        },
        "63f2767d964f453cb3fb83ffec6208e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_affd425ba3364f56b4acadb0bbb924be",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 14/14 [00:21&lt;00:00,  1.55s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1899278a9b1f4fcbbcd9e17c001872d1"
          }
        },
        "f9e21bf730834d5f81d3254dde399915": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "60ec823588c9427cad46b6d0fa074477": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "affd425ba3364f56b4acadb0bbb924be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1899278a9b1f4fcbbcd9e17c001872d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89cf96877ef149049f0ea41fcc856e94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fc269611919c4c2daf56d190dcc3737e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_75a530201ce04b42858fc8a8e9b31c93",
              "IPY_MODEL_dff32c44043f44d69ef3e86ad6c91cbf"
            ]
          }
        },
        "fc269611919c4c2daf56d190dcc3737e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "75a530201ce04b42858fc8a8e9b31c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_56668aeb46a449bfa34ce2c26e4d1927",
            "_dom_classes": [],
            "description": "  6%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 5174,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 329,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_229f647e6a0c4d5191e681c58cedc092"
          }
        },
        "dff32c44043f44d69ef3e86ad6c91cbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_26467429d4d8448093bbfdb152a602f8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 329/5174 [00:03&lt;01:01, 78.95it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6ab59467c6794620a7f0e32b54457912"
          }
        },
        "56668aeb46a449bfa34ce2c26e4d1927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "229f647e6a0c4d5191e681c58cedc092": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "26467429d4d8448093bbfdb152a602f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6ab59467c6794620a7f0e32b54457912": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tushar-semwal/fedperf/blob/main/Santiago/Shakespeare/FedMed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXnVpPCFfQkm"
      },
      "source": [
        "# FedPerf - Shakespeare + FedMed algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWDjzbCzfUFt"
      },
      "source": [
        "## Setup & Dependencies Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_f-rWudW0Fo"
      },
      "source": [
        "%%capture\n",
        "!pip install torchsummaryX unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TmMInb_fnw_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd6e2746-6810-446a-f740-6346e27c79d6"
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "import copy\n",
        "from functools import reduce\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import time\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.sampler import Sampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchsummary import summary\n",
        "from torchsummaryX import summary as summaryx\n",
        "from torchvision import transforms, utils, datasets\n",
        "from tqdm.notebook import tqdm\n",
        "from unidecode import unidecode\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Check assigned GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "# set manual seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# general reproducibility\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed(RANDOM_SEED)\n",
        "\n",
        "# gpu training specific\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Apr 25 22:18:18 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Mfqv6uHfwh-"
      },
      "source": [
        "## Mount GDrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75qbJwxsGj-k"
      },
      "source": [
        "BASE_DIR = '/content/drive/MyDrive/FedPerf/shakespeare/FedMed'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ost-6lXSfveC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e66b4af-6d49-428a-c28a-4994a3b9a5eb"
      },
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    os.makedirs(BASE_DIR, exist_ok=True)\n",
        "except:\n",
        "    print(\"WARNING: Results won't be stored on GDrive\")\n",
        "    BASE_DIR = './'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-ZegBArf4xQ"
      },
      "source": [
        "## Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf03LRxof7Zj"
      },
      "source": [
        "!rm -Rf data\n",
        "!mkdir -p data scripts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngygA4-Fgobx"
      },
      "source": [
        "GENERATE_DATASET = False  # If False, download the dataset provided by the q-FFL paper\n",
        "DATA_DIR = 'data/'\n",
        "# Dataset generation params\n",
        "SAMPLES_FRACTION = 1.  # If using an already generated dataset\n",
        "# SAMPLES_FRACTION = 0.2  # Fraction of total samples in the dataset - FedProx default script\n",
        "# SAMPLES_FRACTION = 0.05  # Fraction of total samples in the dataset - qFFL\n",
        "TRAIN_FRACTION = 0.8  # Train set size\n",
        "MIN_SAMPLES = 0  # Min samples per client (for filtering purposes) - FedProx\n",
        "# MIN_SAMPLES = 64  # Min samples per client (for filtering purposes) - qFFL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUmwJgJygoYD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87612cd4-b22a-4e09-ff28-3f7a7f7782e0"
      },
      "source": [
        "# Download raw dataset\n",
        "# !wget https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt -O data/shakespeare.txt\n",
        "!wget --adjust-extension http://www.gutenberg.org/files/100/100-0.txt -O data/shakespeare.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-25 22:18:39--  http://www.gutenberg.org/files/100/100-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5757108 (5.5M) [text/plain]\n",
            "Saving to: ‘data/shakespeare.txt’\n",
            "\n",
            "\rdata/shakespeare.tx   0%[                    ]       0  --.-KB/s               \rdata/shakespeare.tx 100%[===================>]   5.49M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-04-25 22:18:39 (47.8 MB/s) - ‘data/shakespeare.txt’ saved [5757108/5757108]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dCvx80BgoVr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65047adc-72ca-4f30-e8c7-26d7390ff9ce"
      },
      "source": [
        "if not GENERATE_DATASET:\n",
        "    !rm -Rf data/train data/test\n",
        "    !gdown --id 1n46Mftp3_ahRi1Z6jYhEriyLtdRDS1tD  # Download Shakespeare dataset used by the FedProx paper\n",
        "    !unzip shakespeare.zip\n",
        "    !mv -f shakespeare_paper/train data/\n",
        "    !mv -f shakespeare_paper/test data/\n",
        "    !rm -R shakespeare_paper/ shakespeare.zip\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1n46Mftp3_ahRi1Z6jYhEriyLtdRDS1tD\n",
            "To: /content/shakespeare.zip\n",
            "\r0.00B [00:00, ?B/s]\r2.96MB [00:00, 46.9MB/s]\n",
            "Archive:  shakespeare.zip\n",
            "   creating: shakespeare_paper/\n",
            "   creating: shakespeare_paper/test/\n",
            "  inflating: shakespeare_paper/test/all_data_niid_2_keep_0_test_8.json  \n",
            "   creating: shakespeare_paper/train/\n",
            "  inflating: shakespeare_paper/train/all_data_niid_2_keep_0_train_8.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4pzFvPvhQhq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38501c6d-0548-4a28-b1a7-d8d1c2fb1315"
      },
      "source": [
        "corpus = []\n",
        "with open('data/shakespeare.txt', 'r') as f:\n",
        "    data = list(unidecode(f.read()))\n",
        "    corpus = list(set(list(data)))\n",
        "print('Corpus Length:', len(corpus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus Length: 90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cce_-qnxhD4n"
      },
      "source": [
        "#### Dataset Preprocessing script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt13M4IcgoTV"
      },
      "source": [
        "%%capture\n",
        "if GENERATE_DATASET:\n",
        "    # Download dataset generation scripts\n",
        "    !wget https://raw.githubusercontent.com/ml-lab/FedProx/master/data/shakespeare/preprocess/preprocess_shakespeare.py -O scripts/preprocess_shakespeare.py\n",
        "    !wget https://raw.githubusercontent.com/ml-lab/FedProx/master/data/shakespeare/preprocess/shake_utils.py -O scripts/shake_utils.py\n",
        "    !wget https://raw.githubusercontent.com/ml-lab/FedProx/master/data/shakespeare/preprocess/gen_all_data.py -O scripts/gen_all_data.py\n",
        "\n",
        "    # Download data preprocessing scripts\n",
        "    !wget https://raw.githubusercontent.com/ml-lab/FedProx/master/utils/sample.py -O scripts/sample.py\n",
        "    !wget https://raw.githubusercontent.com/ml-lab/FedProx/master/utils/remove_users.py -O scripts/remove_users.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIEyRW27goPo"
      },
      "source": [
        "# Running scripts\n",
        "if GENERATE_DATASET:\n",
        "    !mkdir -p data/raw_data data/all_data data/train data/test\n",
        "    !python scripts/preprocess_shakespeare.py data/shakespeare.txt data/raw_data\n",
        "    !python scripts/gen_all_data.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq8V6v_4hhhD"
      },
      "source": [
        "#### Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2SjEBKoWDxv"
      },
      "source": [
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, x, y, corpus, seq_length):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.corpus = corpus\n",
        "        self.corpus_size = len(self.corpus)\n",
        "        super(ShakespeareDataset, self).__init__()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__} - (length: {self.__len__()})'\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        input_seq = self.x[i]\n",
        "        next_char = self.y[i]\n",
        "        # print('\\tgetitem', i, input_seq, next_char)\n",
        "        input_value = self.text2charindxs(input_seq)\n",
        "        target_value = self.get_label_from_char(next_char)\n",
        "        return input_value, target_value\n",
        "\n",
        "    def text2charindxs(self, text):\n",
        "        tensor = torch.zeros(len(text), dtype=torch.int32)\n",
        "        for i, c in enumerate(text):\n",
        "            tensor[i] = self.get_label_from_char(c)\n",
        "        return tensor\n",
        "\n",
        "    def get_label_from_char(self, c):\n",
        "        return self.corpus.index(c)\n",
        "\n",
        "    def get_char_from_label(self, l):\n",
        "        return self.corpus[l]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fgJtS62lYAN"
      },
      "source": [
        "##### Federated Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DqL5pTmgn5X"
      },
      "source": [
        "class ShakespeareFedDataset(ShakespeareDataset):\n",
        "    def __init__(self, x, y, corpus, seq_length):\n",
        "        super(ShakespeareFedDataset, self).__init__(x, y, corpus, seq_length)\n",
        "\n",
        "    def dataloader(self, batch_size, shuffle=True):\n",
        "        return DataLoader(self,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=shuffle,\n",
        "                          num_workers=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XelbyPsDlfgb"
      },
      "source": [
        "## Partitioning & Data Loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOBblyFGlwlU"
      },
      "source": [
        "### IID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSZFWKmsgn1p"
      },
      "source": [
        "def iid_partition_(dataset, clients):\n",
        "  \"\"\"\n",
        "  I.I.D paritioning of data over clients\n",
        "  Shuffle the data\n",
        "  Split it between clients\n",
        "  \n",
        "  params:\n",
        "    - dataset (torch.utils.Dataset): Dataset\n",
        "    - clients (int): Number of Clients to split the data between\n",
        "\n",
        "  returns:\n",
        "    - Dictionary of image indexes for each client\n",
        "  \"\"\"\n",
        "\n",
        "  num_items_per_client = int(len(dataset)/clients)\n",
        "  client_dict = {}\n",
        "  image_idxs = [i for i in range(len(dataset))]\n",
        "\n",
        "  for i in range(clients):\n",
        "    client_dict[i] = set(np.random.choice(image_idxs, num_items_per_client, replace=False))\n",
        "    image_idxs = list(set(image_idxs) - client_dict[i])\n",
        "\n",
        "  return client_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lGwDyhSll9h"
      },
      "source": [
        "def iid_partition(corpus, seq_length=80, val_split=False):\n",
        "\n",
        "    train_file = [os.path.join(DATA_DIR, 'train', f) for f in os.listdir(f'{DATA_DIR}/train') if f.endswith('.json')][0]\n",
        "    test_file = [os.path.join(DATA_DIR, 'test', f) for f in os.listdir(f'{DATA_DIR}/test') if f.endswith('.json')][0]\n",
        "\n",
        "    with open(train_file, 'r') as file:\n",
        "        data_train = json.loads(unidecode(file.read()))\n",
        "\n",
        "    with open(test_file, 'r') as file:\n",
        "        data_test = json.loads(unidecode(file.read()))\n",
        "\n",
        "    \n",
        "    total_samples_train = sum(data_train['num_samples'])\n",
        "\n",
        "    data_dict = {}\n",
        "\n",
        "    x_train, y_train = [], []\n",
        "    x_test, y_test = [], []\n",
        "    # x_val, y_val = [], []\n",
        "\n",
        "    users = list(zip(data_train['users'], data_train['num_samples']))\n",
        "    # random.shuffle(users)\n",
        "\n",
        "\n",
        "\n",
        "    total_samples = int(sum(data_train['num_samples']) * SAMPLES_FRACTION)\n",
        "    print('Objective', total_samples, '/', sum(data_train['num_samples']))\n",
        "    sample_count = 0\n",
        "    \n",
        "    for i, (author_id, samples) in enumerate(users):\n",
        "\n",
        "        if sample_count >= total_samples:\n",
        "            print('Max samples reached', sample_count, '/', total_samples)\n",
        "            break\n",
        "\n",
        "        if samples < MIN_SAMPLES: # or data_train['num_samples'][i] > 10000:\n",
        "            print('SKIP', author_id, samples)\n",
        "            continue\n",
        "        else:\n",
        "            udata_train = data_train['user_data'][author_id]\n",
        "            max_samples = samples if (sample_count + samples) <= total_samples else (sample_count + samples - total_samples) \n",
        "            \n",
        "            sample_count += max_samples\n",
        "            # print('sample_count', sample_count)\n",
        "\n",
        "            x_train.extend(data_train['user_data'][author_id]['x'][:max_samples])\n",
        "            y_train.extend(data_train['user_data'][author_id]['y'][:max_samples])\n",
        "\n",
        "            author_data = data_test['user_data'][author_id]\n",
        "            test_size = int(len(author_data['x']) * SAMPLES_FRACTION)\n",
        "\n",
        "            if val_split:\n",
        "                x_test.extend(author_data['x'][:int(test_size / 2)])\n",
        "                y_test.extend(author_data['y'][:int(test_size / 2)])\n",
        "                # x_val.extend(author_data['x'][int(test_size / 2):])\n",
        "                # y_val.extend(author_data['y'][int(test_size / 2):int(test_size)])\n",
        "\n",
        "            else:\n",
        "                x_test.extend(author_data['x'][:int(test_size)])\n",
        "                y_test.extend(author_data['y'][:int(test_size)])\n",
        "\n",
        "    train_ds = ShakespeareDataset(x_train, y_train, corpus, seq_length)\n",
        "    test_ds = ShakespeareDataset(x_test, y_test, corpus, seq_length)\n",
        "    # val_ds = ShakespeareDataset(x_val, y_val, corpus, seq_length)\n",
        "\n",
        "    data_dict = iid_partition_(train_ds, clients=len(users))\n",
        "\n",
        "    return train_ds, data_dict, test_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFvc8mLoouKa"
      },
      "source": [
        "### Non-IID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ76WsCZot9s"
      },
      "source": [
        "def noniid_partition(corpus, seq_length=80, val_split=False):\n",
        "\n",
        "    train_file = [os.path.join(DATA_DIR, 'train', f) for f in os.listdir(f'{DATA_DIR}/train') if f.endswith('.json')][0]\n",
        "    test_file = [os.path.join(DATA_DIR, 'test', f) for f in os.listdir(f'{DATA_DIR}/test') if f.endswith('.json')][0]\n",
        "\n",
        "    with open(train_file, 'r') as file:\n",
        "        data_train = json.loads(unidecode(file.read()))\n",
        "\n",
        "    with open(test_file, 'r') as file:\n",
        "        data_test = json.loads(unidecode(file.read()))\n",
        "\n",
        "    \n",
        "    total_samples_train = sum(data_train['num_samples'])\n",
        "\n",
        "    data_dict = {}\n",
        "\n",
        "    x_test, y_test = [], []\n",
        "\n",
        "    users = list(zip(data_train['users'], data_train['num_samples']))\n",
        "    # random.shuffle(users)\n",
        "\n",
        "    total_samples = int(sum(data_train['num_samples']) * SAMPLES_FRACTION)\n",
        "    print('Objective', total_samples, '/', sum(data_train['num_samples']))\n",
        "    sample_count = 0\n",
        "    \n",
        "    for i, (author_id, samples) in enumerate(users):\n",
        "\n",
        "        if sample_count >= total_samples:\n",
        "            print('Max samples reached', sample_count, '/', total_samples)\n",
        "            break\n",
        "\n",
        "        if samples < MIN_SAMPLES: # or data_train['num_samples'][i] > 10000:\n",
        "            print('SKIP', author_id, samples)\n",
        "            continue\n",
        "        else:\n",
        "            udata_train = data_train['user_data'][author_id]\n",
        "            max_samples = samples if (sample_count + samples) <= total_samples else (sample_count + samples - total_samples) \n",
        "            \n",
        "            sample_count += max_samples\n",
        "            # print('sample_count', sample_count)\n",
        "\n",
        "            x_train = data_train['user_data'][author_id]['x'][:max_samples]\n",
        "            y_train = data_train['user_data'][author_id]['y'][:max_samples]\n",
        "\n",
        "            train_ds = ShakespeareFedDataset(x_train, y_train, corpus, seq_length)\n",
        "\n",
        "            x_val, y_val = None, None\n",
        "            val_ds = None\n",
        "            author_data = data_test['user_data'][author_id]\n",
        "            test_size = int(len(author_data['x']) * SAMPLES_FRACTION)\n",
        "            if val_split:\n",
        "                x_test += author_data['x'][:int(test_size / 2)]\n",
        "                y_test += author_data['y'][:int(test_size / 2)]\n",
        "                x_val = author_data['x'][int(test_size / 2):]\n",
        "                y_val = author_data['y'][int(test_size / 2):int(test_size)]\n",
        "\n",
        "                val_ds = ShakespeareFedDataset(x_val, y_val, corpus, seq_length)\n",
        "\n",
        "            else:\n",
        "                x_test += author_data['x'][:int(test_size)]\n",
        "                y_test += author_data['y'][:int(test_size)]\n",
        "\n",
        "            data_dict[author_id] = {\n",
        "                'train_ds': train_ds,\n",
        "                'val_ds': val_ds\n",
        "            }\n",
        "\n",
        "    test_ds = ShakespeareFedDataset(x_test, y_test, corpus, seq_length)\n",
        "\n",
        "    return data_dict, test_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWVOxcAao2_t"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQQQ2mLeo6EA"
      },
      "source": [
        "### Shakespeare LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mGXTrXRot7R"
      },
      "source": [
        "class ShakespeareLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, classes, lstm_layers=2, dropout=0.1, batch_first=True):\n",
        "        super(ShakespeareLSTM, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.classes = classes\n",
        "        self.no_layers = lstm_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(num_embeddings=self.classes,\n",
        "                                      embedding_dim=self.embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=self.embedding_dim, \n",
        "                            hidden_size=self.hidden_dim,\n",
        "                            num_layers=self.no_layers,\n",
        "                            batch_first=batch_first, \n",
        "                            dropout=dropout if self.no_layers > 1 else 0.)\n",
        "        self.fc = nn.Linear(hidden_dim, self.classes)\n",
        "\n",
        "    def forward(self, x, hc=None):\n",
        "        batch_size = x.size(0)\n",
        "        x_emb = self.embedding(x)\n",
        "        out, (ht, ct) = self.lstm(x_emb.view(batch_size, -1, self.embedding_dim), hc)\n",
        "        dense = self.fc(ht[-1])\n",
        "        return dense\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        return (Variable(torch.zeros(self.no_layers, batch_size, self.hidden_dim)),\n",
        "                Variable(torch.zeros(self.no_layers, batch_size, self.hidden_dim)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QsuJlVipMc8"
      },
      "source": [
        "#### Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_Vb0BYpot5I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ddee073-b3af-412c-e877-70a9896f4083"
      },
      "source": [
        "batch_size = 10\n",
        "seq_length = 80 # mcmahan17a, fedprox, qFFL\n",
        "\n",
        "shakespeare_lstm = ShakespeareLSTM(input_dim=seq_length,  \n",
        "                                   embedding_dim=8,  # mcmahan17a, fedprox, qFFL\n",
        "                                   hidden_dim=256,  # mcmahan17a, fedprox impl\n",
        "                                #    hidden_dim=100,  # fedprox paper\n",
        "                                   classes=len(corpus),\n",
        "                                   lstm_layers=2,\n",
        "                                   dropout=0.1,  # TODO:\n",
        "                                   batch_first=True\n",
        "                                   )\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  shakespeare_lstm.cuda()\n",
        "\n",
        "\n",
        "\n",
        "hc = shakespeare_lstm.init_hidden(batch_size)\n",
        "\n",
        "x_sample = torch.zeros((batch_size, seq_length),\n",
        "                       dtype=torch.long,\n",
        "                       device=(torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')))\n",
        "\n",
        "x_sample[0][0] = 1\n",
        "x_sample\n",
        "\n",
        "print(\"\\nShakespeare LSTM SUMMARY\")\n",
        "print(summaryx(shakespeare_lstm, x_sample))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Shakespeare LSTM SUMMARY\n",
            "==========================================================\n",
            "            Kernel Shape   Output Shape  Params  Mult-Adds\n",
            "Layer                                                     \n",
            "0_embedding      [8, 90]    [10, 80, 8]     720        720\n",
            "1_lstm                 -  [10, 80, 256]  798720     794624\n",
            "2_fc           [256, 90]       [10, 90]   23130      23040\n",
            "----------------------------------------------------------\n",
            "                      Totals\n",
            "Total params          822570\n",
            "Trainable params      822570\n",
            "Non-trainable params       0\n",
            "Mult-Adds             818384\n",
            "==========================================================\n",
            "            Kernel Shape   Output Shape  Params  Mult-Adds\n",
            "Layer                                                     \n",
            "0_embedding      [8, 90]    [10, 80, 8]     720        720\n",
            "1_lstm                 -  [10, 80, 256]  798720     794624\n",
            "2_fc           [256, 90]       [10, 90]   23130      23040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn7egnzTpeks"
      },
      "source": [
        "## FedMed Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFFAfTOwpk4j"
      },
      "source": [
        "### Plot Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyYjWa6IpnTY"
      },
      "source": [
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "367THsiTpo-C"
      },
      "source": [
        "def plot_scores(history, exp_id, title, suffix):\n",
        "    accuracies = [x['accuracy'] for x in history]\n",
        "    f1_macro = [x['f1_macro'] for x in history]\n",
        "    f1_weighted = [x['f1_weighted'] for x in history]\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(accuracies, 'tab:orange')\n",
        "    ax.set(xlabel='Rounds', ylabel='Test Accuracy', title=title)\n",
        "    ax.grid()\n",
        "    fig.savefig(f'{BASE_DIR}/{exp_id}/Test_Accuracy_{suffix}.jpg', format='jpg', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(f1_macro, 'tab:orange')\n",
        "    ax.set(xlabel='Rounds', ylabel='Test F1 (macro)', title=title)\n",
        "    ax.grid()\n",
        "    fig.savefig(f'{BASE_DIR}/{exp_id}/Test_F1_Macro_{suffix}.jpg', format='jpg')\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(f1_weighted, 'tab:orange')\n",
        "    ax.set(xlabel='Rounds', ylabel='Test F1 (weighted)', title=title)\n",
        "    ax.grid()\n",
        "    fig.savefig(f'{BASE_DIR}/{exp_id}/Test_F1_Weighted_{suffix}.jpg', format='jpg')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_losses(history, exp_id, title, suffix):\n",
        "    val_losses = [x['loss'] for x in history]\n",
        "    train_losses = [x['train_loss'] for x in history]\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(train_losses, 'tab:orange')\n",
        "    ax.set(xlabel='Rounds', ylabel='Train Loss', title=title)\n",
        "    ax.grid()\n",
        "    fig.savefig(f'{BASE_DIR}/{exp_id}/Train_Loss_{suffix}.jpg', format='jpg')\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(val_losses, 'tab:orange')\n",
        "    ax.set(xlabel='Rounds', ylabel='Test Loss', title=title)\n",
        "    ax.grid()\n",
        "    fig.savefig(f'{BASE_DIR}/{exp_id}/Test_Loss_{suffix}.jpg', format='jpg')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ9PZM0Gp9ve"
      },
      "source": [
        "### Local Training (Client Update)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDJFltwdotzZ"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, dataset, idxs):\n",
        "      \n",
        "      self.dataset = dataset\n",
        "      self.idxs = list(idxs)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.idxs)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "      data, label = self.dataset[self.idxs[item]]\n",
        "      return data, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtRzU5Yepddq"
      },
      "source": [
        "class ClientUpdate(object):\n",
        "  def __init__(self, dataset, batchSize, learning_rate, epochs, idxs):\n",
        "    # self.train_loader = DataLoader(CustomDataset(dataset, idxs), batch_size=batchSize, shuffle=True)\n",
        "    if hasattr(dataset, 'dataloader'):\n",
        "        self.train_loader = dataset.dataloader(batch_size=batch_size, shuffle=True)\n",
        "    else:\n",
        "        self.train_loader = DataLoader(CustomDataset(dataset, idxs), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.epochs = epochs\n",
        "\n",
        "  def train(self, model):\n",
        "    # print(\"Client training for {} epochs.\".format(self.epochs))\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=self.learning_rate, momentum=0.5)\n",
        "\n",
        "    # use the weights of global model for proximal term calculation\n",
        "    global_model = copy.deepcopy(model)\n",
        "\n",
        "    # calculate local training time\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    e_loss = []\n",
        "    for epoch in range(1, self.epochs+1):\n",
        "\n",
        "      train_loss = 0.0\n",
        "\n",
        "      model.train()\n",
        "      for data, labels in self.train_loader:\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "        # clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # make a forward pass\n",
        "        output = model(data)\n",
        "\n",
        "        loss = criterion(output, labels)\n",
        "        # do a backwards pass\n",
        "        loss.backward()\n",
        "        # perform a single optimization step\n",
        "        optimizer.step()\n",
        "        # update training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "\n",
        "      # average losses\n",
        "      train_loss = train_loss/len(self.train_loader.dataset)\n",
        "      e_loss.append(train_loss)\n",
        "\n",
        "    total_loss = sum(e_loss)/len(e_loss)\n",
        "\n",
        "    return model.state_dict(), total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3crFDN0xqGu6"
      },
      "source": [
        "### Server Side Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c085xSOoqEHk"
      },
      "source": [
        "def training(model, rounds, batch_size, lr, ds, data_dict, test_ds, C, K, E, M, plt_title, plt_color, classes, history=[], eval_every=1, tb_logger=None):\n",
        "  \"\"\"\n",
        "  Function implements the Federated Averaging Algorithm from the FedAvg paper.\n",
        "  Specifically, this function is used for the server side training and weight update\n",
        "\n",
        "  Params:\n",
        "    - model:           PyTorch model to train\n",
        "    - rounds:          Number of communication rounds for the client update\n",
        "    - batch_size:      Batch size for client update training\n",
        "    - lr:              Learning rate used for client update training\n",
        "    - ds:              Dataset used for training\n",
        "    - data_dict:       Type of data partition used for training (IID or non-IID)\n",
        "    - test_data_dict:  Data used for testing the model\n",
        "    - C:               Fraction of clients randomly chosen to perform computation on each round\n",
        "    - K:               Total number of clients\n",
        "    - E:               Number of training passes each client makes over its local dataset per round\n",
        "    - mu:              proximal term constant\n",
        "    - percentage:      percentage of selected client to have fewer than E epochs\n",
        "  Returns:\n",
        "    - model:           Trained model on the server\n",
        "  \"\"\"\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  # global model weights\n",
        "  global_weights = model.state_dict()\n",
        "\n",
        "#   pk = np.ones(K) * 1.0 / K\n",
        "\n",
        "  # training loss\n",
        "  train_loss = []\n",
        "  # test accuracy\n",
        "  test_acc = []\n",
        "\n",
        "  users_id = list(data_dict.keys())\n",
        "\n",
        "  for curr_round in range(1, rounds+1):\n",
        "    w, local_loss = [], []\n",
        "\n",
        "    m = max(int(C*K), 1)\n",
        "    newM = max(int(M*m), 1)\n",
        "\n",
        "    c = 0\n",
        "    S_t = np.random.choice(range(K), m, replace=False)\n",
        "    print('Clients: {}/{} -> {}'.format(len(S_t), K, S_t))\n",
        "\n",
        "    # print('Robustness..')\n",
        "    # for i in range(len(S_t)):\n",
        "    #   if c == newM:\n",
        "    #     break\n",
        "    #   c += 1\n",
        "\n",
        "    #   k = S_t[i]\n",
        "    #   key = users_id[k]\n",
        "    #   ds_ = ds if ds else data_dict[key]['train_ds']\n",
        "    #   idxs = data_dict[key] if ds else None\n",
        "    #   print(f'Client {k}: {len(idxs) if idxs else len(ds_)} samples')\n",
        "    #   local_update = ClientUpdate(dataset=ds_, batchSize=batch_size, learning_rate=lr, epochs=E, idxs=idxs)\n",
        "    #   weights, loss = local_update.train(model=copy.deepcopy(model))\n",
        "\n",
        "    #   for k in weights.keys():\n",
        "    #     t = torch.Tensor(weights[k].shape).cuda()\n",
        "    #     t.fill_(0.1)\n",
        "    #     weights[k] = t\n",
        "\n",
        "    #   w.append(copy.deepcopy(weights))\n",
        "    #   local_loss.append(copy.deepcopy(loss))\n",
        "    # print('###############################')\n",
        "\n",
        "    # for i in tqdm(range(newM, len(S_t))): # if robustness is included\n",
        "    for i in tqdm(range(len(S_t))):\n",
        "      k = S_t[i]\n",
        "      key = users_id[k]\n",
        "      ds_ = ds if ds else data_dict[key]['train_ds']\n",
        "      idxs = data_dict[key] if ds else None\n",
        "      print(f'Client {k}: {len(idxs) if idxs else len(ds_)} samples')\n",
        "      local_update = ClientUpdate(dataset=ds_, batchSize=batch_size, learning_rate=lr, epochs=E, idxs=idxs)\n",
        "      weights, loss = local_update.train(model=copy.deepcopy(model))\n",
        "      \n",
        "      w.append(copy.deepcopy(weights))\n",
        "      local_loss.append(copy.deepcopy(loss))\n",
        "\n",
        "    print('Computing median...')\n",
        "\n",
        "    target = copy.deepcopy(w[0]);\n",
        "    weights_med = copy.deepcopy(w[0]);\n",
        "    for k in weights_med.keys():\n",
        "      tmp = copy.deepcopy(torch.median(torch.stack([w[i][k].data for i in range(0, len(w))]), dim=0))[0]\n",
        "      target[k].data = tmp\n",
        "\n",
        "\n",
        "    global_weights = target\n",
        "    # move the updated weights to our model state dict\n",
        "    model.load_state_dict(global_weights)\n",
        "\n",
        "    # loss\n",
        "    loss_avg = sum(local_loss) / len(local_loss)\n",
        "    print('Round: {}... \\tAverage Loss: {}'.format(curr_round, round(loss_avg, 3)))\n",
        "    train_loss.append(loss_avg)\n",
        "    if tb_logger:\n",
        "        tb_logger.add_scalar(f'Train/Loss', loss_avg, curr_round)\n",
        "\n",
        "    # testing\n",
        "    # if curr_round % eval_every == 0:\n",
        "    test_scores = testing(model, test_ds, batch_size * 2, nn.CrossEntropyLoss(), len(classes), classes)\n",
        "    test_scores['train_loss'] = loss_avg\n",
        "    test_loss, test_accuracy = test_scores['loss'], test_scores['accuracy']\n",
        "    history.append(test_scores)\n",
        "    \n",
        "    # print('Round: {}... \\tAverage Loss: {} \\tTest Loss: {} \\tTest Acc: {}'.format(curr_round, round(loss_avg, 3), round(test_loss, 3), round(test_accuracy, 3)))\n",
        "\n",
        "    if tb_logger:\n",
        "        tb_logger.add_scalar(f'Test/Loss', test_scores['loss'], curr_round)\n",
        "        tb_logger.add_scalars(f'Test/Scores', {\n",
        "            'accuracy': test_scores['accuracy'], 'f1_macro': test_scores['f1_macro'], 'f1_weighted': test_scores['f1_weighted']\n",
        "        }, curr_round)\n",
        "\n",
        "    test_acc.append(test_accuracy)\n",
        "\n",
        "\n",
        "  end = time.time()\n",
        "  \n",
        "  # plot train loss\n",
        "  fig, ax = plt.subplots()\n",
        "  x_axis = np.arange(1, rounds+1)\n",
        "  y_axis = np.array(train_loss)\n",
        "  ax.plot(x_axis, y_axis)\n",
        "\n",
        "  ax.set(xlabel='Number of Rounds', ylabel='Train Loss', title=plt_title)\n",
        "  ax.grid()\n",
        "  # fig.savefig(plt_title+'.jpg', format='jpg')\n",
        "\n",
        "  # plot test accuracy\n",
        "  fig1, ax1 = plt.subplots()\n",
        "  x_axis1 = np.arange(1, rounds+1)\n",
        "  y_axis1 = np.array(test_acc)\n",
        "  ax1.plot(x_axis1, y_axis1)\n",
        "\n",
        "  ax1.set(xlabel='Number of Rounds', ylabel='Test Accuracy', title=plt_title)\n",
        "  ax1.grid()\n",
        "  # fig1.savefig(plt_title+'-test.jpg', format='jpg')\n",
        "  \n",
        "  print(\"Training Done! Total time taken to Train: {}\".format(end-start))\n",
        "\n",
        "  return model, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXtGLkoAqLIW"
      },
      "source": [
        "### Testing Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQJIJno4qKvc"
      },
      "source": [
        "def testing(model, dataset, bs, criterion, num_classes, classes, print_all=False):\n",
        "  #test loss \n",
        "  test_loss = 0.0\n",
        "  correct_class = list(0. for i in range(num_classes))\n",
        "  total_class = list(0. for i in range(num_classes))\n",
        "\n",
        "  test_loader = DataLoader(dataset, batch_size=bs)\n",
        "  l = len(test_loader)\n",
        "  model.eval()\n",
        "  print('running validation...')\n",
        "  for i, (data, labels) in enumerate(tqdm(test_loader)):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    test_loss += loss.item()*data.size(0)\n",
        "\n",
        "    _, pred = torch.max(output, 1)\n",
        "\n",
        "    # For F1Score\n",
        "    y_true = np.append(y_true, labels.data.view_as(pred).cpu().numpy()) if i != 0 else labels.data.view_as(pred).cpu().numpy()\n",
        "    y_hat = np.append(y_hat, pred.cpu().numpy()) if i != 0 else pred.cpu().numpy()\n",
        "\n",
        "    correct_tensor = pred.eq(labels.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
        "\n",
        "    for i, lbl in enumerate(labels.data):\n",
        "    #   print('lbl', i, lbl)\n",
        "      correct_class[lbl] += correct.data[i]\n",
        "      total_class[lbl] += 1\n",
        "    \n",
        "  # avg test loss\n",
        "  test_loss = test_loss/len(test_loader.dataset)\n",
        "  print(\"Test Loss: {:.6f}\\n\".format(test_loss))\n",
        "\n",
        "  # Avg F1 Score\n",
        "  f1_macro = f1_score(y_true, y_hat, average='macro')\n",
        "  # F1-Score -> weigthed to consider class imbalance\n",
        "  f1_weighted =  f1_score(y_true, y_hat, average='weighted')\n",
        "  print(\"F1 Score: {:.6f} (macro) {:.6f} (weighted) %\\n\".format(f1_macro, f1_weighted))\n",
        "\n",
        "  # print test accuracy\n",
        "  if print_all:\n",
        "    for i in range(num_classes):\n",
        "        if total_class[i]>0:\n",
        "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % \n",
        "                    (classes[i], 100 * correct_class[i] / total_class[i],\n",
        "                    np.sum(correct_class[i]), np.sum(total_class[i])))\n",
        "        else:\n",
        "            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "  overall_accuracy = np.sum(correct_class) / np.sum(total_class)\n",
        "\n",
        "  print('\\nFinal Test  Accuracy: {:.3f} ({}/{})'.format(overall_accuracy, np.sum(correct_class), np.sum(total_class)))\n",
        "\n",
        "  return {'loss': test_loss, 'accuracy': overall_accuracy, 'f1_macro': f1_macro, 'f1_weighted': f1_weighted}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxqXLBd8qbC2"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRKlrkVHO8Na",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "2bfbaa4d-27de-46a8-e2d0-50bf0ffd3a74"
      },
      "source": [
        "FAIL-ON-PURPOSE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-cee3c52d72bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mFAIL\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mON\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mPURPOSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'FAIL' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2CfSkNVqKtL"
      },
      "source": [
        "seq_length = 80  # mcmahan17a, fedprox, qFFL\n",
        "embedding_dim = 8  # mcmahan17a, fedprox, qFFL\n",
        "# hidden_dim = 100  # fedprox paper\n",
        "hidden_dim = 256  # mcmahan17a, fedprox impl\n",
        "num_classes = len(corpus)\n",
        "classes = list(range(num_classes))\n",
        "lstm_layers = 2  # mcmahan17a, fedprox, qFFL\n",
        "dropout = 0.1  # TODO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvCgT_qbmFAO"
      },
      "source": [
        "class Hyperparameters():\n",
        "\n",
        "    def __init__(self, total_clients):\n",
        "        # number of training rounds\n",
        "        self.rounds = 50\n",
        "        # client fraction\n",
        "        self.C = 0.1\n",
        "        # number of clients\n",
        "        self.K = total_clients\n",
        "        # number of training passes on local dataset for each roung\n",
        "        # self.E = 20\n",
        "        self.E = 1\n",
        "        self.M = 0.01  # FedMed\n",
        "        # batch size\n",
        "        self.batch_size = 10\n",
        "        # learning Rate\n",
        "        self.lr = 0.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_JVF83mfM3f"
      },
      "source": [
        "exp_log = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYOPtnYoqhWd"
      },
      "source": [
        "### IID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRKc7NrzqKpU"
      },
      "source": [
        "train_ds, data_dict, test_ds = iid_partition(corpus, seq_length, val_split=True)  # Not using val_ds but makes train eval periods faster\n",
        "\n",
        "total_clients = len(data_dict.keys())\n",
        "'Total users:', total_clients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaKtpKT5q1q_"
      },
      "source": [
        "hparams = Hyperparameters(total_clients)\n",
        "hparams.__dict__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ilYMClTV_WR"
      },
      "source": [
        "# Sweeping parameter\n",
        "PARAM_NAME = 'clients_fraction'\n",
        "PARAM_VALUE = hparams.C\n",
        "exp_id = f'{PARAM_NAME}/{PARAM_VALUE}'\n",
        "exp_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAhy4CWVZy3F"
      },
      "source": [
        "EXP_DIR = f'{BASE_DIR}/{exp_id}'\n",
        "os.makedirs(EXP_DIR, exist_ok=True)\n",
        "\n",
        "# tb_logger = SummaryWriter(log_dir)\n",
        "# print(f'TBoard logger created at: {log_dir}')\n",
        "\n",
        "title = 'LSTM FedMed on IID'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwTdeiv8q8_L"
      },
      "source": [
        "def run_experiment(run_id):\n",
        "\n",
        "    shakespeare_lstm = ShakespeareLSTM(input_dim=seq_length,  \n",
        "                                   embedding_dim=embedding_dim,  \n",
        "                                   hidden_dim=hidden_dim,\n",
        "                                   classes=num_classes,\n",
        "                                   lstm_layers=lstm_layers,\n",
        "                                   dropout=dropout,\n",
        "                                   batch_first=True\n",
        "                                   )\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        shakespeare_lstm.cuda()\n",
        "    \n",
        "    test_history = []\n",
        "\n",
        "    lstm_iid_trained, test_history = training(shakespeare_lstm,\n",
        "                                            hparams.rounds, hparams.batch_size, hparams.lr,\n",
        "                                            train_ds,\n",
        "                                            data_dict,\n",
        "                                            test_ds,\n",
        "                                            hparams.C, hparams.K, hparams.E, hparams.M,\n",
        "                                            title, \"green\",\n",
        "                                            corpus, # classes\n",
        "                                            history=test_history,\n",
        "                                            # tb_logger=tb_writer\n",
        "                                            )\n",
        "    \n",
        "\n",
        "    final_scores = testing(lstm_iid_trained, test_ds, batch_size * 2, nn.CrossEntropyLoss(), len(corpus), corpus)\n",
        "    print(f'\\n\\n========================================================\\n\\n')\n",
        "    print(f'Final scores for Exp {run_id} \\n {final_scores}')\n",
        "\n",
        "    return test_history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSU61KsSq87G"
      },
      "source": [
        "exp_history = list()\n",
        "for run_id in range(2):  # TOTAL RUNS\n",
        "    print(f'============== RUNNING EXPERIMENT #{run_id} ==============')\n",
        "    exp_history.append(run_experiment(run_id))\n",
        "    print(f'\\n\\n========================================================\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us-HifGq3Uhf"
      },
      "source": [
        "exp_log[title] = {\n",
        "    'history': exp_history,\n",
        "    'hyperparams': hparams.__dict__\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDGpo4ug33dN"
      },
      "source": [
        "df = None\n",
        "for i, e in enumerate(exp_history):\n",
        "    if i == 0:\n",
        "        df = pd.json_normalize(e)\n",
        "        continue\n",
        "    df = df + pd.json_normalize(e)\n",
        "    \n",
        "df_avg = df / len(exp_history)\n",
        "avg_history = df_avg.to_dict(orient='records')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf77BQAD36Eq"
      },
      "source": [
        "plot_scores(history=avg_history, exp_id=exp_id, title=title, suffix='IID')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJClynRJ38Dh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "60fe7d6c-5782-4e89-8786-ec2a1dddeddc"
      },
      "source": [
        "plot_losses(history=avg_history, exp_id=exp_id, title=title, suffix='IID')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-4c98f353d26c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mavg_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'IID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'avg_history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaoYWkWgqvUQ"
      },
      "source": [
        "### Non-IID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pILgaho8qKgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfaad188-072b-4b7f-fa7d-bfe2f40e1bdb"
      },
      "source": [
        "data_dict, test_ds = noniid_partition(corpus, seq_length=seq_length, val_split=False)\n",
        "\n",
        "total_clients = len(data_dict.keys())\n",
        "'Total users:', total_clients"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Objective 413629 / 413629\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Total users:', 143)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3o7qgBcqKX_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b95fa1b8-766d-459e-9c54-eb68ecca7146"
      },
      "source": [
        "hparams = Hyperparameters(total_clients)\n",
        "hparams.__dict__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 0.1,\n",
              " 'E': 1,\n",
              " 'K': 143,\n",
              " 'M': 0.01,\n",
              " 'batch_size': 10,\n",
              " 'lr': 0.8,\n",
              " 'rounds': 50}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VANr1h0Pq51N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2576581b-0f47-4d16-b096-d23566cb6e4b"
      },
      "source": [
        "# Sweeping parameter\n",
        "PARAM_NAME = 'clients_fraction'\n",
        "PARAM_VALUE = hparams.C\n",
        "exp_id = f'{PARAM_NAME}/{PARAM_VALUE}'\n",
        "exp_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'clients_fraction/0.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXgYFIyZ4ipm"
      },
      "source": [
        "EXP_DIR = f'{BASE_DIR}/{exp_id}'\n",
        "os.makedirs(EXP_DIR, exist_ok=True)\n",
        "\n",
        "# tb_logger = SummaryWriter(log_dir)\n",
        "# print(f'TBoard logger created at: {log_dir}')\n",
        "\n",
        "title = 'LSTM FedMed on Non-IID'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vnv7UaE0q6dG"
      },
      "source": [
        "def run_experiment(run_id):\n",
        "\n",
        "    shakespeare_lstm = ShakespeareLSTM(input_dim=seq_length,  \n",
        "                                   embedding_dim=embedding_dim,  \n",
        "                                   hidden_dim=hidden_dim,\n",
        "                                   classes=num_classes,\n",
        "                                   lstm_layers=lstm_layers,\n",
        "                                   dropout=dropout,\n",
        "                                   batch_first=True\n",
        "                                   )\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        shakespeare_lstm.cuda()\n",
        "    \n",
        "    test_history = []\n",
        "\n",
        "    lstm_iid_trained, test_history = training(shakespeare_lstm,\n",
        "                                            hparams.rounds, hparams.batch_size, hparams.lr,\n",
        "                                            None,\n",
        "                                            data_dict,\n",
        "                                            test_ds,\n",
        "                                            hparams.C, hparams.K, hparams.E, hparams.M,\n",
        "                                            title, \"green\",\n",
        "                                            corpus, # classes\n",
        "                                            history=test_history,\n",
        "                                            # tb_logger=tb_writer\n",
        "                                            )\n",
        "    \n",
        "\n",
        "    final_scores = testing(lstm_iid_trained, test_ds, batch_size * 2, nn.CrossEntropyLoss(), len(corpus), corpus)\n",
        "    print(f'\\n\\n========================================================\\n\\n')\n",
        "    print(f'Final scores for Exp {run_id} \\n {final_scores}')\n",
        "\n",
        "    return test_history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pLbVBwVq6Uw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475,
          "referenced_widgets": [
            "6d797b2da75d40cc9c17a482bfa2733c",
            "9e037649af1b47d5887e494f42df9e23",
            "09e8bd023b73490a82de6a39fa2c3262",
            "63f2767d964f453cb3fb83ffec6208e6",
            "f9e21bf730834d5f81d3254dde399915",
            "60ec823588c9427cad46b6d0fa074477",
            "affd425ba3364f56b4acadb0bbb924be",
            "1899278a9b1f4fcbbcd9e17c001872d1",
            "89cf96877ef149049f0ea41fcc856e94",
            "fc269611919c4c2daf56d190dcc3737e",
            "75a530201ce04b42858fc8a8e9b31c93",
            "dff32c44043f44d69ef3e86ad6c91cbf",
            "56668aeb46a449bfa34ce2c26e4d1927",
            "229f647e6a0c4d5191e681c58cedc092",
            "26467429d4d8448093bbfdb152a602f8",
            "6ab59467c6794620a7f0e32b54457912"
          ]
        },
        "outputId": "20853d4d-d475-4fa2-ca21-68d7d6b4127f"
      },
      "source": [
        "exp_history = list()\n",
        "for run_id in range(2):  # TOTAL RUNS\n",
        "    print(f'============== RUNNING EXPERIMENT #{run_id} ==============')\n",
        "    exp_history.append(run_experiment(run_id))\n",
        "    print(f'\\n\\n========================================================\\n\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============== RUNNING EXPERIMENT #0 ==============\n",
            "Clients: 14/143 -> [ 40  49  54  44   2  71  80 102 135  69 117  70  79  55]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d797b2da75d40cc9c17a482bfa2733c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=14.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Client 40: 1081 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:662: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Client 49: 1335 samples\n",
            "Client 54: 398 samples\n",
            "Client 44: 2487 samples\n",
            "Client 2: 929 samples\n",
            "Client 71: 174 samples\n",
            "Client 80: 28 samples\n",
            "Client 102: 6587 samples\n",
            "Client 135: 159 samples\n",
            "Client 69: 4694 samples\n",
            "Client 117: 31 samples\n",
            "Client 70: 236 samples\n",
            "Client 79: 196 samples\n",
            "Client 55: 245 samples\n",
            "\n",
            "Computing median...\n",
            "Round: 1... \tAverage Loss: 3.615\n",
            "running validation...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89cf96877ef149049f0ea41fcc856e94",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=5174.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5F38z5C4qw9"
      },
      "source": [
        "exp_log[title] = {\n",
        "    'history': exp_history,\n",
        "    'hyperparams': hparams.__dict__\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inIGn3Mh4qpO"
      },
      "source": [
        "df = None\n",
        "for i, e in enumerate(exp_history):\n",
        "    if i == 0:\n",
        "        df = pd.json_normalize(e)\n",
        "        continue\n",
        "    df = df + pd.json_normalize(e)\n",
        "    \n",
        "df_avg = df / len(exp_history)\n",
        "avg_history = df_avg.to_dict(orient='records')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8ngcls64qjc"
      },
      "source": [
        "plot_scores(history=avg_history, exp_id=exp_id, title=title, suffix='nonIID')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR9vjtYs4qBX"
      },
      "source": [
        "plot_losses(history=avg_history, exp_id=exp_id, title=title, suffix='nonIID')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adK1OTS-40Z8"
      },
      "source": [
        "### Pickle Experiment Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5nl-hsa4zqw"
      },
      "source": [
        "import pickle\n",
        " \n",
        "with open(f'{EXP_DIR}/results.pkl', 'wb') as file:\n",
        "    pickle.dump(exp_log, file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}